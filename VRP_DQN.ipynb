{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VRP_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4exWwXUGiJek"
      ],
      "authorship_tag": "ABX9TyOkv5cpVnoUGq94+cN0+xA+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baudouindetruchis/VRP-RL/blob/main/VRP_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIJ2sUAryfsm"
      },
      "source": [
        "**Ressources**\r\n",
        "1. [AlphaZero paper](https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ)\r\n",
        "2. [How to build AlphaZero algo](https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188)\r\n",
        "3. [Colab tips](https://towardsdatascience.com/10-tips-for-a-better-google-colab-experience-33f8fe721b82)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4o9xEUP61xS"
      },
      "source": [
        "**Ideas**\r\n",
        "1. Optimize types (float < **int**)\r\n",
        "2. Transfer learning for feature extraction --> speed-up learning\r\n",
        "3. Reduce action space (decrease nn task complexity)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxmJqiK3qc5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7abfc4eb-2af2-42b7-ba6a-7c1461f25b14"
      },
      "source": [
        "import numpy as np\r\n",
        "import random\r\n",
        "import os\r\n",
        "import time\r\n",
        "from collections import deque\r\n",
        "from keras.models import Sequential, load_model, Model\r\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Activation, ZeroPadding2D, Add, Input\r\n",
        "from keras.utils.vis_utils import plot_model\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import tensorflow as tf\r\n",
        "from keras.callbacks import TensorBoard\r\n",
        "\r\n",
        "# Check GPU\r\n",
        "if not len(tf.test.gpu_device_name()):\r\n",
        "    print(\"[INFO] GPU not activated\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] GPU not activated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4exWwXUGiJek"
      },
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE5WyNfNiLIk"
      },
      "source": [
        "# Load tensorboard notebook extension (with magic command %)\r\n",
        "%load_ext tensorboard\r\n",
        "\r\n",
        "# Own Tensorboard class\r\n",
        "class ModifiedTensorBoard(TensorBoard):\r\n",
        "\r\n",
        "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.step = 1\r\n",
        "        # self.writer = tf.summary.FileWriter(self.log_dir)\r\n",
        "        self.writer = tf.summary.create_file_writer(self.log_dir)\r\n",
        "\r\n",
        "    # Overriding this method to stop creating default log writer\r\n",
        "    def set_model(self, model):\r\n",
        "        pass\r\n",
        "\r\n",
        "    # Overrided, saves logs with our step number\r\n",
        "    # (otherwise every .fit() will start writing from 0th step)\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        self.update_stats(**logs)\r\n",
        "\r\n",
        "    # Overrided\r\n",
        "    # We train for one batch only, no need to save anything at epoch end\r\n",
        "    def on_batch_end(self, batch, logs=None):\r\n",
        "        pass\r\n",
        "\r\n",
        "    # Overrided, so won't close writer\r\n",
        "    def on_train_end(self, _):\r\n",
        "        pass\r\n",
        "\r\n",
        "    # Custom method for saving own metrics\r\n",
        "    # Creates writer, writes custom metrics and closes writer\r\n",
        "    def update_stats(self, **stats):\r\n",
        "        self._write_logs(stats, self.step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBUbbHsEDO60"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBgqgBRDOKsm"
      },
      "source": [
        "##### ENVIRONMENT\r\n",
        "GRID_SIZE = 10\r\n",
        "N_POD = 15\r\n",
        "\r\n",
        "##### SELF PLAY\r\n",
        "EPISODES = 20_000\r\n",
        "DISCOUNT = 0.99\r\n",
        "EPSILON = 1     # variable which is going to be decayed\r\n",
        "EPSILON_DECAY = 0.99975\r\n",
        "MIN_EPSILON = 0.001\r\n",
        "\r\n",
        "##### MODEL\r\n",
        "LOAD_MODEL = None  # Filepath or None\r\n",
        "MODEL_NAME = 'RES_3x256C'\r\n",
        "HISTORY_SIZE = 2\r\n",
        "\r\n",
        "##### RETRAINING\r\n",
        "REPLAY_MEMORY_SIZE = 50_000\r\n",
        "MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\r\n",
        "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\r\n",
        "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\r\n",
        "\r\n",
        "##### STATS MONITORING\r\n",
        "MIN_REWARD = -65   # For model save\r\n",
        "AGGREGATE_STATS_EVERY = 50      # Episodes"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdXwRW7lOSYf"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu13h5l_qimN"
      },
      "source": [
        "class VRP:\r\n",
        "    def __init__(self, grid_size=GRID_SIZE, n_pod=N_POD, history_size=HISTORY_SIZE):\r\n",
        "        # Parameters\r\n",
        "        self.grid_size = grid_size\r\n",
        "        self.n_pod = n_pod\r\n",
        "        self.history_size = history_size\r\n",
        "\r\n",
        "        # Game states\r\n",
        "        self.depot_grid = self.create_depot_grid()\r\n",
        "        self.pod_grid = self.create_pod_grid()\r\n",
        "        self.history = np.concatenate((np.expand_dims(self.depot_grid, axis=2), np.zeros((self.grid_size, self.grid_size, history_size), dtype=int)), axis=2)\r\n",
        "        \r\n",
        "        # Variables\r\n",
        "        self.episode_step = 0\r\n",
        "        self.route = [np.unravel_index(np.argmax(self.depot_grid, axis=None), self.depot_grid.shape)]     # Stores vehicule route\r\n",
        "\r\n",
        "    # Give the position of the depot\r\n",
        "    def create_depot_grid(self):\r\n",
        "        depot_grid = np.zeros((self.grid_size, self.grid_size), dtype=int)\r\n",
        "        depot_grid[np.random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)] = 1\r\n",
        "        return depot_grid\r\n",
        "\r\n",
        "    # Give position of PoD\r\n",
        "    def create_pod_grid(self):\r\n",
        "        pod_grid = np.zeros((self.grid_size, self.grid_size), dtype=int)\r\n",
        "        for i in range(self.n_pod):\r\n",
        "            random_x = np.random.randint(0, self.grid_size - 1)\r\n",
        "            random_y = np.random.randint(0, self.grid_size - 1)\r\n",
        "            # Reset if spot already taken\r\n",
        "            while (self.depot_grid[random_x, random_y] == 1) or (pod_grid[random_x, random_y] == 1):\r\n",
        "                random_x = np.random.randint(0, self.grid_size - 1)\r\n",
        "                random_y = np.random.randint(0, self.grid_size - 1)\r\n",
        "            pod_grid[random_x, random_y] = 1\r\n",
        "        return pod_grid\r\n",
        "    \r\n",
        "    def get_observation(self):\r\n",
        "        # Stack (create a new dimension) depot and PoD grid\r\n",
        "        observation = np.concatenate((np.expand_dims(self.depot_grid, axis=2), np.expand_dims(self.pod_grid, axis=2)), axis=2)\r\n",
        "        observation = np.concatenate((observation, self.history), axis=2)\r\n",
        "        return observation\r\n",
        "    \r\n",
        "    # Return coordinates of the max valid action\r\n",
        "    def max_valid_action(self, prediction):\r\n",
        "        # Valid actions: not yet visited PoD\r\n",
        "        valid_actions = (self.depot_grid + self.pod_grid - self.history[:,:,0]).astype(bool)\r\n",
        "        prediction[~valid_actions] = -np.inf    # NN can output negative values\r\n",
        "        max_valid_coordinates = np.unravel_index(np.argmax(prediction, axis=None), prediction.shape)    # Argmax in n-dimension array\r\n",
        "        return max_valid_coordinates\r\n",
        "    \r\n",
        "    # Prediction post-processing: return closest point in a direction\r\n",
        "    def prediction_processing(self, prediction):\r\n",
        "        \r\n",
        "        return action\r\n",
        "    \r\n",
        "    # Total distance as the crow flies\r\n",
        "    def get_total_distance(self):\r\n",
        "        distance = 0\r\n",
        "        for i in range(len(env.route)-1):\r\n",
        "            distance += np.sqrt((env.route[i+1][0]-env.route[i][0])**2 + (env.route[i+1][1]-env.route[i][1])**2)\r\n",
        "        return round(distance, 1)\r\n",
        "    \r\n",
        "    # Last step distance\r\n",
        "    def get_step_distance(self):\r\n",
        "        distance = np.sqrt((env.route[-1][0]-env.route[-2][0])**2 + (env.route[-1][1]-env.route[-2][1])**2)\r\n",
        "        return round(distance, 1)\r\n",
        "    \r\n",
        "    def step(self, action):\r\n",
        "        self.episode_step += 1\r\n",
        "        # Update game states\r\n",
        "        new_state = self.history[:,:,0].copy()\r\n",
        "        new_state[action[0], action[1]] = 1\r\n",
        "        self.history = np.concatenate((np.expand_dims(new_state, axis=2), self.history[:,:,:-1]), axis=2).copy()\r\n",
        "        # Save position for distance calculation\r\n",
        "        current_position = self.history[:,:,0] - self.history[:,:,1]\r\n",
        "        self.route.append(np.unravel_index(np.argmax(current_position, axis=None), current_position.shape))\r\n",
        "        # Check if episode finished\r\n",
        "        if (self.depot_grid + self.pod_grid == self.history[:,:,0]).all():\r\n",
        "            done = True\r\n",
        "            self.route.append(np.unravel_index(np.argmax(self.depot_grid, axis=None), self.depot_grid.shape))\r\n",
        "            reward = - (self.get_step_distance() + self.get_total_distance())\r\n",
        "        else:\r\n",
        "            done = False\r\n",
        "            reward = -self.get_step_distance()\r\n",
        "        return self.get_observation(), reward, done\r\n",
        "\r\n",
        "env = VRP()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0xV5qiBOWh5"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTSi9CiLK7RW"
      },
      "source": [
        "class Agent:\r\n",
        "    def __init__(self):\r\n",
        "        # Main model (train every step)\r\n",
        "        self.model = self.create_model()\r\n",
        "        # Target network (.predict every step --> updated to main model after X complete episode)\r\n",
        "        self.target_model = self.create_model()\r\n",
        "        self.target_model.set_weights(self.model.get_weights())\r\n",
        "        # An array with last n steps for training (create a batch for updates)\r\n",
        "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\r\n",
        "        # Used to count when to update target network with main network's weights\r\n",
        "        self.target_update_counter = 0\r\n",
        "        # Custom tensorboard object\r\n",
        "        # self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{MODEL_NAME}_{int(time.time())}\")\r\n",
        "    \r\n",
        "    def create_model(self):\r\n",
        "        if LOAD_MODEL:\r\n",
        "            print(f\"[INFO] Loading model: {LOAD_MODEL}\")\r\n",
        "            model = load_model(LOAD_MODEL)\r\n",
        "            print(\"[INFO] Model loaded\")\r\n",
        "        else:\r\n",
        "            X_input = Input(shape=(env.grid_size, env.grid_size, 3 + env.history_size))\r\n",
        "\r\n",
        "            X = ZeroPadding2D((2, 2))(X_input)      # Zero-Padding to keep borders info\r\n",
        "\r\n",
        "            X_shortcut = X      # Save the input value\r\n",
        "\r\n",
        "            X = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(X)\r\n",
        "            X = BatchNormalization()(X)  # Applied after non linearity (other say before)\r\n",
        "            X = Dropout(0.1)(X)\r\n",
        "\r\n",
        "            X = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(X)\r\n",
        "            X = BatchNormalization()(X)\r\n",
        "            X = Dropout(0.1)(X)\r\n",
        "\r\n",
        "            X = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(X)\r\n",
        "            X = BatchNormalization()(X)\r\n",
        "            X = Dropout(0.1)(X)\r\n",
        "\r\n",
        "            # Shortcut path\r\n",
        "            X_shortcut = Conv2D(256, (1, 1), padding='same')(X_shortcut)\r\n",
        "            X_shortcut = BatchNormalization()(X_shortcut)\r\n",
        "\r\n",
        "            X = Add()([X, X_shortcut])\r\n",
        "            X = Activation('relu')(X)\r\n",
        "\r\n",
        "            X = Flatten()(X)  # this converts our 3D feature maps to 1D feature vectors\r\n",
        "            X = Dense(256)(X)\r\n",
        "            X = BatchNormalization()(X)\r\n",
        "            X = Dense((8), activation='linear')(X)   # Output = (north, east, south, west)\r\n",
        "\r\n",
        "            model = Model(inputs=X_input, outputs=X, name=MODEL_NAME)\r\n",
        "            model.compile(loss=\"mse\", optimizer = \"adam\", metrics=['accuracy'])\r\n",
        "\r\n",
        "        return model\r\n",
        "\r\n",
        "    # Adds step's data to a memory replay array (observation, action, reward, new_observation, done)\r\n",
        "    def update_replay_memory(self, transition):\r\n",
        "        self.replay_memory.append(transition)\r\n",
        "    \r\n",
        "    def train(self, terminal_state, step):\r\n",
        "        # Start training only if certain number of samples is already saved\r\n",
        "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\r\n",
        "            return\r\n",
        "\r\n",
        "        # Get a minibatch of random samples from memory replay table\r\n",
        "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\r\n",
        "        # Get current states from minibatch, then query NN model for Q values\r\n",
        "        current_states = np.array([transition[0] for transition in minibatch])\r\n",
        "        current_qs_list = self.model.predict(current_states)\r\n",
        "        # Get future states from minibatch, then query NN model for Q values\r\n",
        "        # When using target network, query it, otherwise main network should be queried\r\n",
        "        new_current_states = np.array([transition[3] for transition in minibatch])\r\n",
        "        future_qs_list = self.target_model.predict(new_current_states)\r\n",
        "        X = []\r\n",
        "        y = []\r\n",
        "\r\n",
        "        # Now we need to enumerate our batches\r\n",
        "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\r\n",
        "            # If not a terminal state, get new q from future states, otherwise set it to 0\r\n",
        "            # almost like with Q Learning, but we use just part of equation here\r\n",
        "            if not done:\r\n",
        "                max_future_q = np.max(future_qs_list[index])\r\n",
        "                new_q = reward + DISCOUNT * max_future_q\r\n",
        "            else:\r\n",
        "                new_q = reward\r\n",
        "            # Update Q value for given state\r\n",
        "            current_qs = current_qs_list[index]\r\n",
        "            current_qs[action] = new_q\r\n",
        "            # And append to our training data\r\n",
        "            X.append(current_state)\r\n",
        "            y.append(current_qs)\r\n",
        "\r\n",
        "        # Fit on all samples as one batch, log only on terminal state\r\n",
        "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)\r\n",
        "                    #    , callbacks=[self.tensorboard] if terminal_state else None)\r\n",
        "        # Update target network counter every episode\r\n",
        "        if terminal_state:\r\n",
        "            self.target_update_counter += 1\r\n",
        "        # If counter reaches set value, update target network with weights of main network\r\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\r\n",
        "            self.target_model.set_weights(self.model.get_weights())\r\n",
        "            self.target_update_counter = 0\r\n",
        "\r\n",
        "    def get_qs(self, state):\r\n",
        "        #### DEBUG ####\r\n",
        "        # return np.random.uniform(size=(env.grid_size, env.grid_size))\r\n",
        "        ###############\r\n",
        "        return self.model.predict(state.reshape(-1, *state.shape))[0]\r\n",
        "\r\n",
        "agent = Agent()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXzGOPGEOaWr"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UllXDsZmGYZI"
      },
      "source": [
        "# For repetitive results\r\n",
        "np.random.seed(1234)\r\n",
        "random.seed(1234)\r\n",
        "\r\n",
        "# Create models folder\r\n",
        "if not os.path.isdir('models'):\r\n",
        "    os.makedirs('models')\r\n",
        "\r\n",
        "# For statistics\r\n",
        "reward_stats = []\r\n",
        "\r\n",
        "# Iterate over episodes\r\n",
        "for episode in tqdm(range(1, EPISODES + 1), unit='episodes', desc='Training'):\r\n",
        "    # Update tensorboard step every episode\r\n",
        "    # agent.tensorboard.step = episode\r\n",
        "    # Reset episode parameters\r\n",
        "    episode_reward = 0\r\n",
        "    step = 1\r\n",
        "    done = False\r\n",
        "    # Reset environment + get initial state\r\n",
        "    env = VRP()\r\n",
        "    current_state = env.get_observation()\r\n",
        "\r\n",
        "    # Iterate until episode ends\r\n",
        "    while not done:\r\n",
        "        # This part stays mostly the same, the change is to query a model for Q values\r\n",
        "        if np.random.random() > EPSILON:\r\n",
        "            # Get action from Q table\r\n",
        "            action = env.max_valid_action(agent.get_qs(current_state))\r\n",
        "        else:\r\n",
        "            # Get random action\r\n",
        "            action = env.max_valid_action(np.random.uniform(size=(env.grid_size, env.grid_size)))\r\n",
        "\r\n",
        "        new_state, reward, done = env.step(action)\r\n",
        "        episode_reward += reward\r\n",
        "        # Every step we update replay memory and train main network\r\n",
        "        agent.update_replay_memory((current_state, action, reward, new_state, done))\r\n",
        "        agent.train(done, step)\r\n",
        "        # Update episode parameters\r\n",
        "        current_state = new_state\r\n",
        "        step += 1\r\n",
        "\r\n",
        "    # Append episode reward to a list and log stats (every given number of episodes)\r\n",
        "    reward_stats.append(episode_reward)\r\n",
        "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\r\n",
        "        average_reward = sum(reward_stats[-AGGREGATE_STATS_EVERY:])/len(reward_stats[-AGGREGATE_STATS_EVERY:])\r\n",
        "        min_reward = min(reward_stats[-AGGREGATE_STATS_EVERY:])\r\n",
        "        max_reward = max(reward_stats[-AGGREGATE_STATS_EVERY:])\r\n",
        "        # agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\r\n",
        "\r\n",
        "        # Save model, but only when min reward is greater or equal a set value\r\n",
        "        if min_reward >= MIN_REWARD:\r\n",
        "            agent.model.save(f'models/{MODEL_NAME}__{round(average_reward,1)}avg__{int(time.time())}.model')\r\n",
        "        print(f\"[INFO] episode: {episode} -- AVG reward: {average_reward}\")\r\n",
        "\r\n",
        "    # Decay epsilon\r\n",
        "    if EPSILON > MIN_EPSILON:\r\n",
        "        EPSILON *= EPSILON_DECAY\r\n",
        "        EPSILON = max(MIN_EPSILON, EPSILON)\r\n",
        "\r\n",
        "# Save final version\r\n",
        "agent.model.save(f'models/{MODEL_NAME}__{round(average_reward,1)}avg__{int(time.time())}.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce7cLQKKYcA5"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0EB7nC_YaEj"
      },
      "source": [
        "# %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvkza30rYiXq"
      },
      "source": [
        "## History"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd8hpIFoV_OY"
      },
      "source": [
        "| date | version | performance | comment |\r\n",
        "| :--- | :--- | :--- | :--- |\r\n",
        "| 07/01 | 2x256C | -75 (10x10 15PoD) | first trainable model but had a bug (not converging after 30h) |\r\n",
        "| 13/01 | 2x256C fixed | -75 (10x10 15PoD) | not converging but only 2K episodes |\r\n",
        "| 13/01 | 3x256C + 1x128 | -75 (10x10 15PoD) | not converging |\r\n",
        "| 14/01 | 3x256C + 1x356 + res | -75 (10x10 15PoD) | not converging |\r\n",
        "| 30/01 | 3x256C + 1x356 + res | ??? | trying less complex action space |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neYyvtDZ1AQF"
      },
      "source": [
        "## Sandbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j9APB4MKgiH"
      },
      "source": [
        "# for i in range(1):\r\n",
        "#     # Reset environment\r\n",
        "#     env = VRP(grid_size=10, n_pod=10)\r\n",
        "#     done = False\r\n",
        "\r\n",
        "#     # for i in range(100):\r\n",
        "#     while done == False:\r\n",
        "#         action = env.max_valid_action(agent.get_qs(env.get_observation()))\r\n",
        "#         # print(action)\r\n",
        "#         observation, reward, done = env.step(action)\r\n",
        "#         print(observation[:,:,2], '\\n')\r\n",
        "#         if reward != 0:\r\n",
        "#             # print(observation)\r\n",
        "#             print(reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7HfmQ4yHGOj",
        "outputId": "23e5871e-f00e-4050-ef54-0db478b1f120"
      },
      "source": [
        "env.get_observation()[:,:,0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fMkIEm0G9_P",
        "outputId": "3b934e20-e654-494a-88cf-6153201ba700"
      },
      "source": [
        "env.get_observation()[:,:,1]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 1, 1, 0, 1, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP7iagcQdmNO",
        "outputId": "047f5d72-2ed6-4666-bd93-5a28ba3dff56"
      },
      "source": [
        "# Current truck position\r\n",
        "current_position = env.history[:,:,0] - env.history[:,:,1]\r\n",
        "current_coordinates = np.unravel_index(np.argmax(current_position, axis=None), current_position.shape)\r\n",
        "# Get valid actions coordinates\r\n",
        "valid_actions = (env.depot_grid + env.pod_grid - env.history[:,:,0])\r\n",
        "valid_coordinates = np.argwhere(valid_actions == 1)\r\n",
        "print(valid_coordinates)\r\n",
        "# Transform into polar coordinates\r\n",
        "offset_coordinates = valid_coordinates.copy()\r\n",
        "offset_coordinates[:,0] = valid_coordinates[:,1] - current_coordinates[1]\r\n",
        "offset_coordinates[:,1] = -valid_coordinates[:,0] + current_coordinates[0]\r\n",
        "polar_coordinates = offset_coordinates.astype(float)\r\n",
        "polar_coordinates[:,0] = np.round(np.sqrt(offset_coordinates[:,0]**2 + offset_coordinates[:,1]**2), 2)\r\n",
        "polar_coordinates[:,1] = np.round(np.arctan2(offset_coordinates[:,1], offset_coordinates[:,0]), 2)\r\n",
        "print(polar_coordinates)\r\n",
        "# Select closest point in a direction\r\n",
        "prediction = np.array([0,0,0,1,0,0,0,0])\r\n",
        "i = np.argmax(prediction)\r\n",
        "direction_indexes = np.argwhere(((polar_coordinates[:,1] > (i*np.pi/4 - np.pi/4)) \\\r\n",
        "                                    & (polar_coordinates[:,1] < (i*np.pi/4 + np.pi/4))) \\\r\n",
        "                                | ((polar_coordinates[:,1] + 2*np.pi > (i*np.pi/4 - np.pi/4)) \\\r\n",
        "                                    & (polar_coordinates[:,1] + 2*np.pi < (i*np.pi/4 + np.pi/4))))\r\n",
        "direction_coordinates = valid_coordinates[direction_indexes.flatten()]\r\n",
        "print(direction_coordinates)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 2]\n",
            " [1 1]\n",
            " [2 2]\n",
            " [2 6]\n",
            " [3 1]\n",
            " [3 2]\n",
            " [3 4]\n",
            " [3 6]\n",
            " [4 7]\n",
            " [5 0]\n",
            " [5 7]\n",
            " [6 6]\n",
            " [7 1]\n",
            " [7 2]\n",
            " [8 3]]\n",
            "[[ 2.83  2.36]\n",
            " [ 3.16  2.82]\n",
            " [ 2.    3.14]\n",
            " [ 2.    0.  ]\n",
            " [ 3.16 -2.82]\n",
            " [ 2.24 -2.68]\n",
            " [ 1.   -1.57]\n",
            " [ 2.24 -0.46]\n",
            " [ 3.61 -0.59]\n",
            " [ 5.   -2.5 ]\n",
            " [ 4.24 -0.79]\n",
            " [ 4.47 -1.11]\n",
            " [ 5.83 -2.11]\n",
            " [ 5.39 -1.95]\n",
            " [ 6.08 -1.74]]\n",
            "[[0 2]\n",
            " [1 1]\n",
            " [2 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0watvDeG8g-f",
        "outputId": "e328494f-ae1e-438a-acbe-76a6d11122a2"
      },
      "source": [
        "direction_indexes"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjhsbSVUHDYS",
        "outputId": "d620113f-32a2-41c6-ba4a-037ca4cc2f55"
      },
      "source": [
        "polar_coordinates[:,1]%(np.pi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.6       , 2.16      , 1.57      , 2.94      , 2.68      ,\n",
              "       0.46      , 3.14      , 0.        , 0.        , 1.11159265,\n",
              "       0.64159265, 2.16159265, 0.67159265, 1.81159265, 2.03159265])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7raFwDs_Hfj4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}