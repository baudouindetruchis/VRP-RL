{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VRP_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4exWwXUGiJek"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP162gllFvgnDBPDvijsTet",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11d96f84b0ca470aa385f640b8185b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_89a5cd9bd573492caf575b7692b154be",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aa42a0c5ad0d47cf9894fe85d5b6f2e3",
              "IPY_MODEL_31e13caecffc4021ac1e90549bf3036e"
            ]
          }
        },
        "89a5cd9bd573492caf575b7692b154be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa42a0c5ad0d47cf9894fe85d5b6f2e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_23167fd1a91a4fe49bc23ac31b803522",
            "_dom_classes": [],
            "description": "Training:  40%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 20000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7960,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a4cd5c53aee4ac9af8ef06499b72461"
          }
        },
        "31e13caecffc4021ac1e90549bf3036e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_13e800ab1a494f3b8ed648c70f19b310",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7960/20000 [4:48:45&lt;7:54:41,  2.37s/episodes]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_44f3e0850b3545fcb362bf827f0c6dfa"
          }
        },
        "23167fd1a91a4fe49bc23ac31b803522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a4cd5c53aee4ac9af8ef06499b72461": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13e800ab1a494f3b8ed648c70f19b310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "44f3e0850b3545fcb362bf827f0c6dfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baudouindetruchis/VRP-RL/blob/main/VRP_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIJ2sUAryfsm"
      },
      "source": [
        "**Ressources**\r\n",
        "1. [AlphaZero paper](https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ)\r\n",
        "2. [How to build AlphaZero algo](https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188)\r\n",
        "3. [Colab tips](https://towardsdatascience.com/10-tips-for-a-better-google-colab-experience-33f8fe721b82)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4o9xEUP61xS"
      },
      "source": [
        "**Ideas**\r\n",
        "1. Optimize types (float < **int**)\r\n",
        "2. Transfer learning for feature extraction --> speed-up learning\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxmJqiK3qc5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697f7ad7-ad73-4ed5-bce2-41ceca8c1464"
      },
      "source": [
        "import numpy as np\r\n",
        "import random\r\n",
        "import os\r\n",
        "import time\r\n",
        "from collections import deque\r\n",
        "from keras.models import Sequential, load_model, Model\r\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Activation, ZeroPadding2D, Add, Input\r\n",
        "from keras.utils.vis_utils import plot_model\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import tensorflow as tf\r\n",
        "from keras.callbacks import TensorBoard\r\n",
        "\r\n",
        "\r\n",
        "# Load tensorboard notebook extension (with magic command %)\r\n",
        "# %load_ext tensorboard\r\n",
        "\r\n",
        "# Check GPU\r\n",
        "if not len(tf.test.gpu_device_name()):\r\n",
        "    print(\"[INFO] GPU not activated\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4exWwXUGiJek"
      },
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE5WyNfNiLIk"
      },
      "source": [
        "# Own Tensorboard class\r\n",
        "class ModifiedTensorBoard(TensorBoard):\r\n",
        "\r\n",
        "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.step = 1\r\n",
        "        # self.writer = tf.summary.FileWriter(self.log_dir)\r\n",
        "        self.writer = tf.summary.create_file_writer(self.log_dir)\r\n",
        "\r\n",
        "    # Overriding this method to stop creating default log writer\r\n",
        "    def set_model(self, model):\r\n",
        "        pass\r\n",
        "\r\n",
        "    # Overrided, saves logs with our step number\r\n",
        "    # (otherwise every .fit() will start writing from 0th step)\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        self.update_stats(**logs)\r\n",
        "\r\n",
        "    # Overrided\r\n",
        "    # We train for one batch only, no need to save anything at epoch end\r\n",
        "    def on_batch_end(self, batch, logs=None):\r\n",
        "        pass\r\n",
        "\r\n",
        "    # Overrided, so won't close writer\r\n",
        "    def on_train_end(self, _):\r\n",
        "        pass\r\n",
        "\r\n",
        "    # Custom method for saving own metrics\r\n",
        "    # Creates writer, writes custom metrics and closes writer\r\n",
        "    def update_stats(self, **stats):\r\n",
        "        self._write_logs(stats, self.step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBUbbHsEDO60"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBgqgBRDOKsm"
      },
      "source": [
        "##### ENVIRONMENT\r\n",
        "GRID_SIZE = 10\r\n",
        "N_POD = 15\r\n",
        "\r\n",
        "##### SELF PLAY\r\n",
        "EPISODES = 20_000\r\n",
        "DISCOUNT = 0.99\r\n",
        "EPSILON = 1     # variable which is going to be decayed\r\n",
        "EPSILON_DECAY = 0.99975\r\n",
        "MIN_EPSILON = 0.001\r\n",
        "\r\n",
        "##### MODEL\r\n",
        "LOAD_MODEL = None  # Filepath or None\r\n",
        "MODEL_NAME = '3x256C'\r\n",
        "HISTORY_SIZE = 2\r\n",
        "\r\n",
        "##### RETRAINING\r\n",
        "REPLAY_MEMORY_SIZE = 50_000\r\n",
        "MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\r\n",
        "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\r\n",
        "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\r\n",
        "\r\n",
        "##### STATS MONITORING\r\n",
        "MIN_REWARD = -65   # For model save\r\n",
        "AGGREGATE_STATS_EVERY = 50      # episodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdXwRW7lOSYf"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu13h5l_qimN"
      },
      "source": [
        "class VRP:\r\n",
        "    def __init__(self, grid_size=GRID_SIZE, n_pod=N_POD, history_size=HISTORY_SIZE):\r\n",
        "        # Parameters\r\n",
        "        self.grid_size = grid_size\r\n",
        "        self.n_pod = n_pod\r\n",
        "        self.history_size = history_size\r\n",
        "\r\n",
        "        # Game states\r\n",
        "        self.depot_grid = self.create_depot_grid()\r\n",
        "        self.pod_grid = self.create_pod_grid()\r\n",
        "        self.history = np.concatenate((np.expand_dims(self.depot_grid, axis=2), np.zeros((self.grid_size, self.grid_size, history_size), dtype=int)), axis=2)\r\n",
        "        \r\n",
        "        # Variables\r\n",
        "        self.episode_step = 0\r\n",
        "        self.route = [np.unravel_index(np.argmax(self.depot_grid, axis=None), self.depot_grid.shape)]     # Stores vehicule route\r\n",
        "\r\n",
        "    # Give the position of the depot\r\n",
        "    def create_depot_grid(self):\r\n",
        "        depot_grid = np.zeros((self.grid_size, self.grid_size), dtype=int)\r\n",
        "        depot_grid[np.random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)] = 1\r\n",
        "        return depot_grid\r\n",
        "\r\n",
        "    # Give position of PoD\r\n",
        "    def create_pod_grid(self):\r\n",
        "        pod_grid = np.zeros((self.grid_size, self.grid_size), dtype=int)\r\n",
        "        for i in range(self.n_pod):\r\n",
        "            random_x = np.random.randint(0, self.grid_size - 1)\r\n",
        "            random_y = np.random.randint(0, self.grid_size - 1)\r\n",
        "            # Reset if spot already taken\r\n",
        "            while (self.depot_grid[random_x, random_y] == 1) or (pod_grid[random_x, random_y] == 1):\r\n",
        "                random_x = np.random.randint(0, self.grid_size - 1)\r\n",
        "                random_y = np.random.randint(0, self.grid_size - 1)\r\n",
        "            pod_grid[random_x, random_y] = 1\r\n",
        "        return pod_grid\r\n",
        "    \r\n",
        "    def get_observation(self):\r\n",
        "        # Stack (create a new dimension) depot and PoD grid\r\n",
        "        observation = np.concatenate((np.expand_dims(self.depot_grid, axis=2), np.expand_dims(self.pod_grid, axis=2)), axis=2)\r\n",
        "        observation = np.concatenate((observation, self.history), axis=2)\r\n",
        "        return observation\r\n",
        "    \r\n",
        "    # Return coordinates of the max valid action\r\n",
        "    def max_valid_action(self, prediction):\r\n",
        "        # Valid actions: not yet visited PoD\r\n",
        "        valid_actions = (self.depot_grid + self.pod_grid - self.history[:,:,0]).astype(bool)\r\n",
        "        prediction[~valid_actions] = -np.inf    # NN can output negative values\r\n",
        "        max_valid_coordinates = np.unravel_index(np.argmax(prediction, axis=None), prediction.shape)    # Argmax in n-dimension array\r\n",
        "        return max_valid_coordinates\r\n",
        "    \r\n",
        "    # Distance as the crow flies\r\n",
        "    def get_total_distance(self):\r\n",
        "        distance = 0\r\n",
        "        for i in range(len(env.route)-1):\r\n",
        "            distance += np.sqrt((env.route[i+1][0]-env.route[i][0])**2 + (env.route[i+1][1]-env.route[i][1])**2)\r\n",
        "        return round(distance, 1)\r\n",
        "    \r\n",
        "    # Last step distance\r\n",
        "    def get_step_distance(self):\r\n",
        "        distance = np.sqrt((env.route[-1][0]-env.route[-2][0])**2 + (env.route[-1][1]-env.route[-2][1])**2)\r\n",
        "        return round(distance, 1)\r\n",
        "    \r\n",
        "    def step(self, action):\r\n",
        "        self.episode_step += 1\r\n",
        "        # Update game states\r\n",
        "        new_state = self.history[:,:,0].copy()\r\n",
        "        new_state[action[0], action[1]] = 1\r\n",
        "        self.history = np.concatenate((np.expand_dims(new_state, axis=2), self.history[:,:,:-1]), axis=2).copy()\r\n",
        "        # Save position for distance calculation\r\n",
        "        current_position = self.history[:,:,0] - self.history[:,:,1]\r\n",
        "        self.route.append(np.unravel_index(np.argmax(current_position, axis=None), current_position.shape))\r\n",
        "        # Check if episode finished\r\n",
        "        if (self.depot_grid + self.pod_grid == self.history[:,:,0]).all():\r\n",
        "            done = True\r\n",
        "            self.route.append(np.unravel_index(np.argmax(self.depot_grid, axis=None), self.depot_grid.shape))\r\n",
        "            reward = - (self.get_step_distance() + self.get_total_distance())\r\n",
        "        else:\r\n",
        "            done = False\r\n",
        "            reward = -self.get_step_distance()\r\n",
        "        return self.get_observation(), reward, done\r\n",
        "\r\n",
        "env = VRP()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0xV5qiBOWh5"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTSi9CiLK7RW"
      },
      "source": [
        "class Agent:\r\n",
        "    def __init__(self):\r\n",
        "        # Main model (train every step)\r\n",
        "        self.model = self.create_model()\r\n",
        "        # Target network (.predict every step --> updated to main model after X complete episode)\r\n",
        "        self.target_model = self.create_model()\r\n",
        "        self.target_model.set_weights(self.model.get_weights())\r\n",
        "\r\n",
        "        # An array with last n steps for training (create a batch for updates)\r\n",
        "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\r\n",
        "        # Used to count when to update target network with main network's weights\r\n",
        "        self.target_update_counter = 0\r\n",
        "        # Custom tensorboard object\r\n",
        "        # self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{MODEL_NAME}_{int(time.time())}\")\r\n",
        "    \r\n",
        "    def create_model(self):\r\n",
        "        if LOAD_MODEL:\r\n",
        "            print(f\"[INFO] Loading model: {LOAD_MODEL}\")\r\n",
        "            model = load_model(LOAD_MODEL)\r\n",
        "            print(\"[INFO] Model loaded\")\r\n",
        "        else:\r\n",
        "            X_input = Input(shape=(env.grid_size, env.grid_size, 3 + env.history_size))\r\n",
        "\r\n",
        "            X = ZeroPadding2D((2, 2))(X_input)      # Zero-Padding to keep borders info\r\n",
        "\r\n",
        "            X_shortcut = X      # Save the input value\r\n",
        "\r\n",
        "            X = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(X)\r\n",
        "            X = BatchNormalization()(X)  # Applied after non linearity (other say before)\r\n",
        "            X = Dropout(0.1)(X)\r\n",
        "\r\n",
        "            X = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(X)\r\n",
        "            X = BatchNormalization()(X)\r\n",
        "            X = Dropout(0.1)(X)\r\n",
        "\r\n",
        "            X = Conv2D(256, (3, 3), padding = 'same', activation = 'relu')(X)\r\n",
        "            X = BatchNormalization()(X)\r\n",
        "            X = Dropout(0.1)(X)\r\n",
        "\r\n",
        "            # Shortcut path\r\n",
        "            X_shortcut = Conv2D(256, (1, 1), padding='same')(X_shortcut)\r\n",
        "            X_shortcut = BatchNormalization()(X_shortcut)\r\n",
        "\r\n",
        "            X = Add()([X, X_shortcut])\r\n",
        "            X = Activation('relu')(X)\r\n",
        "\r\n",
        "            X = Flatten()(X)  # this converts our 3D feature maps to 1D feature vectors\r\n",
        "            X = Dense(256)(X)\r\n",
        "            X = BatchNormalization()(X)\r\n",
        "            X = Dense((env.grid_size**2), activation='linear')(X)   # Output\r\n",
        "\r\n",
        "            model = Model(inputs=X_input, outputs=X, name=MODEL_NAME)\r\n",
        "            model.compile(loss=\"mse\", optimizer = \"adam\", metrics=['accuracy'])\r\n",
        "            \r\n",
        "        return model\r\n",
        "\r\n",
        "    # Adds step's data to a memory replay array (observation, action, reward, new_observation, done)\r\n",
        "    def update_replay_memory(self, transition):\r\n",
        "        self.replay_memory.append(transition)\r\n",
        "    \r\n",
        "    def train(self, terminal_state, step):\r\n",
        "        # Start training only if certain number of samples is already saved\r\n",
        "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\r\n",
        "            return\r\n",
        "\r\n",
        "        # Get a minibatch of random samples from memory replay table\r\n",
        "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\r\n",
        "        # Get current states from minibatch, then query NN model for Q values\r\n",
        "        current_states = np.array([transition[0] for transition in minibatch])\r\n",
        "        current_qs_list = self.model.predict(current_states).reshape(-1, env.grid_size, env.grid_size)\r\n",
        "        # Get future states from minibatch, then query NN model for Q values\r\n",
        "        # When using target network, query it, otherwise main network should be queried\r\n",
        "        new_current_states = np.array([transition[3] for transition in minibatch])\r\n",
        "        future_qs_list = self.target_model.predict(new_current_states).reshape(-1, env.grid_size, env.grid_size)\r\n",
        "        X = []\r\n",
        "        y = []\r\n",
        "\r\n",
        "        # Now we need to enumerate our batches\r\n",
        "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\r\n",
        "            # If not a terminal state, get new q from future states, otherwise set it to 0\r\n",
        "            # almost like with Q Learning, but we use just part of equation here\r\n",
        "            if not done:\r\n",
        "                max_future_q = np.max(future_qs_list[index])\r\n",
        "                new_q = reward + DISCOUNT * max_future_q\r\n",
        "            else:\r\n",
        "                new_q = reward\r\n",
        "            # Update Q value for given state\r\n",
        "            current_qs = current_qs_list[index]\r\n",
        "            current_qs[action] = new_q\r\n",
        "            # And append to our training data\r\n",
        "            X.append(current_state)\r\n",
        "            y.append(current_qs.flatten())\r\n",
        "\r\n",
        "        # Fit on all samples as one batch, log only on terminal state\r\n",
        "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)\r\n",
        "                    #    , callbacks=[self.tensorboard] if terminal_state else None)\r\n",
        "        # Update target network counter every episode\r\n",
        "        if terminal_state:\r\n",
        "            self.target_update_counter += 1\r\n",
        "        # If counter reaches set value, update target network with weights of main network\r\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\r\n",
        "            self.target_model.set_weights(self.model.get_weights())\r\n",
        "            self.target_update_counter = 0\r\n",
        "\r\n",
        "    def get_qs(self, state):\r\n",
        "        #### DEBUG ####\r\n",
        "        # return np.random.uniform(size=(env.grid_size, env.grid_size))\r\n",
        "        ###############\r\n",
        "        return self.model.predict(state.reshape(-1, *state.shape))[0].reshape(env.grid_size, env.grid_size)\r\n",
        "\r\n",
        "agent = Agent()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXzGOPGEOaWr"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UllXDsZmGYZI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "11d96f84b0ca470aa385f640b8185b79",
            "89a5cd9bd573492caf575b7692b154be",
            "aa42a0c5ad0d47cf9894fe85d5b6f2e3",
            "31e13caecffc4021ac1e90549bf3036e",
            "23167fd1a91a4fe49bc23ac31b803522",
            "6a4cd5c53aee4ac9af8ef06499b72461",
            "13e800ab1a494f3b8ed648c70f19b310",
            "44f3e0850b3545fcb362bf827f0c6dfa"
          ]
        },
        "outputId": "64b65032-0070-4ffa-fec0-f8705ac13fa9"
      },
      "source": [
        "# For repetitive results\r\n",
        "np.random.seed(1234)\r\n",
        "random.seed(1234)\r\n",
        "\r\n",
        "# Create models folder\r\n",
        "if not os.path.isdir('models'):\r\n",
        "    os.makedirs('models')\r\n",
        "\r\n",
        "# For statistics\r\n",
        "reward_stats = []\r\n",
        "\r\n",
        "# Iterate over episodes\r\n",
        "for episode in tqdm(range(1, EPISODES + 1), unit='episodes', desc='Training'):\r\n",
        "    # Update tensorboard step every episode\r\n",
        "    # agent.tensorboard.step = episode\r\n",
        "    # Reset episode parameters\r\n",
        "    episode_reward = 0\r\n",
        "    step = 1\r\n",
        "    done = False\r\n",
        "    # Reset environment + get initial state\r\n",
        "    env = VRP()\r\n",
        "    current_state = env.get_observation()\r\n",
        "\r\n",
        "    # Iterate until episode ends\r\n",
        "    while not done:\r\n",
        "        # This part stays mostly the same, the change is to query a model for Q values\r\n",
        "        if np.random.random() > EPSILON:\r\n",
        "            # Get action from Q table\r\n",
        "            action = env.max_valid_action(agent.get_qs(current_state))\r\n",
        "        else:\r\n",
        "            # Get random action\r\n",
        "            action = env.max_valid_action(np.random.uniform(size=(env.grid_size, env.grid_size)))\r\n",
        "\r\n",
        "        new_state, reward, done = env.step(action)\r\n",
        "        episode_reward += reward\r\n",
        "        # Every step we update replay memory and train main network\r\n",
        "        agent.update_replay_memory((current_state, action, reward, new_state, done))\r\n",
        "        agent.train(done, step)\r\n",
        "        # Update episode parameters\r\n",
        "        current_state = new_state\r\n",
        "        step += 1\r\n",
        "\r\n",
        "    # Append episode reward to a list and log stats (every given number of episodes)\r\n",
        "    reward_stats.append(episode_reward)\r\n",
        "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\r\n",
        "        average_reward = sum(reward_stats[-AGGREGATE_STATS_EVERY:])/len(reward_stats[-AGGREGATE_STATS_EVERY:])\r\n",
        "        min_reward = min(reward_stats[-AGGREGATE_STATS_EVERY:])\r\n",
        "        max_reward = max(reward_stats[-AGGREGATE_STATS_EVERY:])\r\n",
        "        # agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\r\n",
        "\r\n",
        "        # Save model, but only when min reward is greater or equal a set value\r\n",
        "        if min_reward >= MIN_REWARD:\r\n",
        "            agent.model.save(f'models/{MODEL_NAME}__{round(average_reward,1)}avg__{int(time.time())}.model')\r\n",
        "        print(f\"[INFO] episode: {episode} -- AVG reward: {average_reward}\")\r\n",
        "\r\n",
        "\r\n",
        "    # Decay epsilon\r\n",
        "    if EPSILON > MIN_EPSILON:\r\n",
        "        EPSILON *= EPSILON_DECAY\r\n",
        "        EPSILON = max(MIN_EPSILON, EPSILON)\r\n",
        "\r\n",
        "# Save final version\r\n",
        "agent.model.save(f'models/{MODEL_NAME}__{round(average_reward,1)}avg__{int(time.time())}.model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11d96f84b0ca470aa385f640b8185b79",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Training', max=20000.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] episode: 1 -- AVG reward: -156.0\n",
            "[INFO] episode: 50 -- AVG reward: -144.77800000000005\n",
            "[INFO] episode: 100 -- AVG reward: -151.32600000000002\n",
            "[INFO] episode: 150 -- AVG reward: -144.54399999999998\n",
            "[INFO] episode: 200 -- AVG reward: -148.114\n",
            "[INFO] episode: 250 -- AVG reward: -145.75400000000002\n",
            "[INFO] episode: 300 -- AVG reward: -151.43\n",
            "[INFO] episode: 350 -- AVG reward: -145.62800000000001\n",
            "[INFO] episode: 400 -- AVG reward: -146.79799999999994\n",
            "[INFO] episode: 450 -- AVG reward: -146.91000000000005\n",
            "[INFO] episode: 500 -- AVG reward: -146.85199999999998\n",
            "[INFO] episode: 550 -- AVG reward: -150.362\n",
            "[INFO] episode: 600 -- AVG reward: -151.12800000000004\n",
            "[INFO] episode: 650 -- AVG reward: -148.87199999999996\n",
            "[INFO] episode: 700 -- AVG reward: -148.91200000000003\n",
            "[INFO] episode: 750 -- AVG reward: -144.54399999999998\n",
            "[INFO] episode: 800 -- AVG reward: -144.78400000000002\n",
            "[INFO] episode: 850 -- AVG reward: -145.404\n",
            "[INFO] episode: 900 -- AVG reward: -142.87400000000002\n",
            "[INFO] episode: 950 -- AVG reward: -141.21\n",
            "[INFO] episode: 1000 -- AVG reward: -148.832\n",
            "[INFO] episode: 1050 -- AVG reward: -143.106\n",
            "[INFO] episode: 1100 -- AVG reward: -147.112\n",
            "[INFO] episode: 1150 -- AVG reward: -152.30399999999995\n",
            "[INFO] episode: 1200 -- AVG reward: -146.04599999999996\n",
            "[INFO] episode: 1250 -- AVG reward: -148.502\n",
            "[INFO] episode: 1300 -- AVG reward: -150.00600000000003\n",
            "[INFO] episode: 1350 -- AVG reward: -151.664\n",
            "[INFO] episode: 1400 -- AVG reward: -150.40599999999998\n",
            "[INFO] episode: 1450 -- AVG reward: -139.40800000000002\n",
            "[INFO] episode: 1500 -- AVG reward: -148.08199999999997\n",
            "[INFO] episode: 1550 -- AVG reward: -142.90999999999997\n",
            "[INFO] episode: 1600 -- AVG reward: -142.996\n",
            "[INFO] episode: 1650 -- AVG reward: -145.78\n",
            "[INFO] episode: 1700 -- AVG reward: -148.17\n",
            "[INFO] episode: 1750 -- AVG reward: -146.43000000000004\n",
            "[INFO] episode: 1800 -- AVG reward: -145.07400000000004\n",
            "[INFO] episode: 1850 -- AVG reward: -148.7\n",
            "[INFO] episode: 1900 -- AVG reward: -143.588\n",
            "[INFO] episode: 1950 -- AVG reward: -143.20799999999997\n",
            "[INFO] episode: 2000 -- AVG reward: -147.27199999999996\n",
            "[INFO] episode: 2050 -- AVG reward: -142.26200000000003\n",
            "[INFO] episode: 2100 -- AVG reward: -150.50000000000003\n",
            "[INFO] episode: 2150 -- AVG reward: -145.264\n",
            "[INFO] episode: 2200 -- AVG reward: -150.18200000000002\n",
            "[INFO] episode: 2250 -- AVG reward: -143.99399999999991\n",
            "[INFO] episode: 2300 -- AVG reward: -143.874\n",
            "[INFO] episode: 2350 -- AVG reward: -145.05\n",
            "[INFO] episode: 2400 -- AVG reward: -145.67599999999996\n",
            "[INFO] episode: 2450 -- AVG reward: -149.872\n",
            "[INFO] episode: 2500 -- AVG reward: -149.82399999999998\n",
            "[INFO] episode: 2550 -- AVG reward: -148.20800000000006\n",
            "[INFO] episode: 2600 -- AVG reward: -143.888\n",
            "[INFO] episode: 2650 -- AVG reward: -146.678\n",
            "[INFO] episode: 2700 -- AVG reward: -148.304\n",
            "[INFO] episode: 2750 -- AVG reward: -145.85600000000005\n",
            "[INFO] episode: 2800 -- AVG reward: -147.40999999999994\n",
            "[INFO] episode: 2850 -- AVG reward: -146.584\n",
            "[INFO] episode: 2900 -- AVG reward: -146.66000000000003\n",
            "[INFO] episode: 2950 -- AVG reward: -144.87600000000003\n",
            "[INFO] episode: 3000 -- AVG reward: -146.136\n",
            "[INFO] episode: 3050 -- AVG reward: -147.36800000000005\n",
            "[INFO] episode: 3100 -- AVG reward: -142.02999999999997\n",
            "[INFO] episode: 3150 -- AVG reward: -140.888\n",
            "[INFO] episode: 3200 -- AVG reward: -144.72400000000005\n",
            "[INFO] episode: 3250 -- AVG reward: -148.44199999999998\n",
            "[INFO] episode: 3300 -- AVG reward: -143.438\n",
            "[INFO] episode: 3350 -- AVG reward: -147.466\n",
            "[INFO] episode: 3400 -- AVG reward: -144.966\n",
            "[INFO] episode: 3450 -- AVG reward: -145.60200000000003\n",
            "[INFO] episode: 3500 -- AVG reward: -140.84800000000004\n",
            "[INFO] episode: 3550 -- AVG reward: -149.28799999999998\n",
            "[INFO] episode: 3600 -- AVG reward: -147.70199999999997\n",
            "[INFO] episode: 3650 -- AVG reward: -149.56399999999996\n",
            "[INFO] episode: 3700 -- AVG reward: -143.882\n",
            "[INFO] episode: 3750 -- AVG reward: -149.37999999999997\n",
            "[INFO] episode: 3800 -- AVG reward: -144.762\n",
            "[INFO] episode: 3850 -- AVG reward: -149.92400000000004\n",
            "[INFO] episode: 3900 -- AVG reward: -145.34199999999998\n",
            "[INFO] episode: 3950 -- AVG reward: -145.60800000000006\n",
            "[INFO] episode: 4000 -- AVG reward: -149.96800000000002\n",
            "[INFO] episode: 4050 -- AVG reward: -146.16600000000003\n",
            "[INFO] episode: 4100 -- AVG reward: -150.35400000000004\n",
            "[INFO] episode: 4150 -- AVG reward: -147.628\n",
            "[INFO] episode: 4200 -- AVG reward: -143.99200000000005\n",
            "[INFO] episode: 4250 -- AVG reward: -146.73799999999997\n",
            "[INFO] episode: 4300 -- AVG reward: -143.69799999999998\n",
            "[INFO] episode: 4350 -- AVG reward: -145.58\n",
            "[INFO] episode: 4400 -- AVG reward: -147.55799999999996\n",
            "[INFO] episode: 4450 -- AVG reward: -144.11399999999995\n",
            "[INFO] episode: 4500 -- AVG reward: -145.43800000000002\n",
            "[INFO] episode: 4550 -- AVG reward: -143.02400000000003\n",
            "[INFO] episode: 4600 -- AVG reward: -146.486\n",
            "[INFO] episode: 4650 -- AVG reward: -143.28799999999998\n",
            "[INFO] episode: 4700 -- AVG reward: -145.42600000000002\n",
            "[INFO] episode: 4750 -- AVG reward: -146.93200000000002\n",
            "[INFO] episode: 4800 -- AVG reward: -142.136\n",
            "[INFO] episode: 4850 -- AVG reward: -146.42800000000003\n",
            "[INFO] episode: 4900 -- AVG reward: -147.98000000000005\n",
            "[INFO] episode: 4950 -- AVG reward: -145.29999999999998\n",
            "[INFO] episode: 5000 -- AVG reward: -147.63799999999995\n",
            "[INFO] episode: 5050 -- AVG reward: -144.276\n",
            "[INFO] episode: 5100 -- AVG reward: -146.80599999999998\n",
            "[INFO] episode: 5150 -- AVG reward: -143.02\n",
            "[INFO] episode: 5200 -- AVG reward: -144.77800000000002\n",
            "[INFO] episode: 5250 -- AVG reward: -146.91000000000003\n",
            "[INFO] episode: 5300 -- AVG reward: -144.054\n",
            "[INFO] episode: 5350 -- AVG reward: -143.89599999999996\n",
            "[INFO] episode: 5400 -- AVG reward: -147.882\n",
            "[INFO] episode: 5450 -- AVG reward: -144.42999999999998\n",
            "[INFO] episode: 5500 -- AVG reward: -147.97800000000004\n",
            "[INFO] episode: 5550 -- AVG reward: -142.26799999999997\n",
            "[INFO] episode: 5600 -- AVG reward: -144.738\n",
            "[INFO] episode: 5650 -- AVG reward: -147.56199999999998\n",
            "[INFO] episode: 5700 -- AVG reward: -147.22200000000004\n",
            "[INFO] episode: 5750 -- AVG reward: -147.61599999999996\n",
            "[INFO] episode: 5800 -- AVG reward: -149.14800000000002\n",
            "[INFO] episode: 5850 -- AVG reward: -151.61999999999998\n",
            "[INFO] episode: 5900 -- AVG reward: -143.31000000000003\n",
            "[INFO] episode: 5950 -- AVG reward: -147.89600000000004\n",
            "[INFO] episode: 6000 -- AVG reward: -147.546\n",
            "[INFO] episode: 6050 -- AVG reward: -151.424\n",
            "[INFO] episode: 6100 -- AVG reward: -144.138\n",
            "[INFO] episode: 6150 -- AVG reward: -143.55599999999995\n",
            "[INFO] episode: 6200 -- AVG reward: -143.81400000000002\n",
            "[INFO] episode: 6250 -- AVG reward: -145.44600000000003\n",
            "[INFO] episode: 6300 -- AVG reward: -142.98600000000002\n",
            "[INFO] episode: 6350 -- AVG reward: -143.562\n",
            "[INFO] episode: 6400 -- AVG reward: -146.34199999999998\n",
            "[INFO] episode: 6450 -- AVG reward: -143.028\n",
            "[INFO] episode: 6500 -- AVG reward: -148.21200000000007\n",
            "[INFO] episode: 6550 -- AVG reward: -145.014\n",
            "[INFO] episode: 6600 -- AVG reward: -149.11199999999997\n",
            "[INFO] episode: 6650 -- AVG reward: -145.162\n",
            "[INFO] episode: 6700 -- AVG reward: -147.306\n",
            "[INFO] episode: 6750 -- AVG reward: -146.71400000000003\n",
            "[INFO] episode: 6800 -- AVG reward: -150.55999999999997\n",
            "[INFO] episode: 6850 -- AVG reward: -146.10399999999996\n",
            "[INFO] episode: 6900 -- AVG reward: -145.01400000000004\n",
            "[INFO] episode: 6950 -- AVG reward: -145.37400000000002\n",
            "[INFO] episode: 7000 -- AVG reward: -146.76\n",
            "[INFO] episode: 7050 -- AVG reward: -145.71799999999996\n",
            "[INFO] episode: 7100 -- AVG reward: -145.26\n",
            "[INFO] episode: 7150 -- AVG reward: -145.32199999999997\n",
            "[INFO] episode: 7200 -- AVG reward: -147.70799999999994\n",
            "[INFO] episode: 7250 -- AVG reward: -145.106\n",
            "[INFO] episode: 7300 -- AVG reward: -148.02599999999998\n",
            "[INFO] episode: 7350 -- AVG reward: -147.46600000000004\n",
            "[INFO] episode: 7400 -- AVG reward: -145.45399999999998\n",
            "[INFO] episode: 7450 -- AVG reward: -145.974\n",
            "[INFO] episode: 7500 -- AVG reward: -146.05399999999997\n",
            "[INFO] episode: 7550 -- AVG reward: -144.332\n",
            "[INFO] episode: 7600 -- AVG reward: -147.998\n",
            "[INFO] episode: 7650 -- AVG reward: -146.886\n",
            "[INFO] episode: 7700 -- AVG reward: -146.40399999999997\n",
            "[INFO] episode: 7750 -- AVG reward: -150.04000000000005\n",
            "[INFO] episode: 7800 -- AVG reward: -146.476\n",
            "[INFO] episode: 7850 -- AVG reward: -140.99200000000002\n",
            "[INFO] episode: 7900 -- AVG reward: -142.958\n",
            "[INFO] episode: 7950 -- AVG reward: -138.84199999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce7cLQKKYcA5"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0EB7nC_YaEj"
      },
      "source": [
        "# %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvkza30rYiXq"
      },
      "source": [
        "## History"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd8hpIFoV_OY"
      },
      "source": [
        "| date | version | performance | comment |\r\n",
        "| :--- | :--- | :--- | :--- |\r\n",
        "| 07/01 | 2x256C | -75 (10x10 15PoD) | first trainable model but had a bug (not converging after 30h) |\r\n",
        "| 13/01 | 2x256C fixed | -75 (10x10 15PoD) | not converging but only 2K episodes |\r\n",
        "| 13/01 | 3x256C + 1x128 | -75 (10x10 15PoD) | not converging |\r\n",
        "| 14/01 | 3x256C + 1x356 + res | -75 (10x10 15PoD) | ??? |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neYyvtDZ1AQF"
      },
      "source": [
        "## Sandbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j9APB4MKgiH"
      },
      "source": [
        "# for i in range(1):\r\n",
        "#     # Reset environment\r\n",
        "#     env = VRP(grid_size=10, n_pod=10)\r\n",
        "#     done = False\r\n",
        "\r\n",
        "#     # for i in range(100):\r\n",
        "#     while done == False:\r\n",
        "#         action = env.max_valid_action(agent.get_qs(env.get_observation()))\r\n",
        "#         # print(action)\r\n",
        "#         observation, reward, done = env.step(action)\r\n",
        "#         print(observation[:,:,2], '\\n')\r\n",
        "#         if reward != 0:\r\n",
        "#             # print(observation)\r\n",
        "#             print(reward)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}